# 过度拟合的问题(The Problem of Overfitting)

过度拟合表现在图像上就是**曲线经过了每一个数据点，但整个曲线不是一个<font color=ff0099>凸曲线</font>，而是一个波浪线。  
<font color=00cccc>过度拟合的问题出现于存在过多变量的时候，并且得到的函数也不能推广到新的样本的应用或预测。简单来说，过度拟合的函数可以<font size=4 color=ff0099>严丝合缝</font>的拟合训练集，但是<font size=4 color=ff0099>无法应用在新数据的预测上</font>。</font>
<br />
<br />
<br />

## 解决途径

- 尝试减少特征变量的数量。我们可以人为观察变量集，继而决定哪些变量更重要，哪些变量应该保留或舍弃。
- 模型选择算法(Model selection algorithm)。这种算法可以迅速判断哪些变量应该保留、哪些变量应该舍弃。

但减少变量的代价就是损失了一部分主题的信息。所以还有以下的方法来解决这个棘手的问题：
<font color=ff0099 size=4>
- 正则化(Regularization)。在这个方法中，会保留所有的变量，但会减少$\theta_j$的大小值。当主题拥有大量的特征变量时，这种方法可以运作的很好，每一个变量都会为预测Y的值作出一点贡献，每一个变量都多少有点作用。

</font>
<br />
<br />
<br />

# 代价函数(Cost Function)

我们已经知道，适当的包含有二次项的函数(例如：$\theta_0+\theta_1x+\theta_2x^2$)可以较好的拟合数据而不会产生过度拟合的问题，而拥有三次项甚至四次项的函数(例如：$\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$)就容易出现过度拟合的问题。  

解决的思路是尽量使函数中的三次项和四次项<font color=00cccc size=3>接近于0</font>：  

$$min\frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+ 1000\theta_3^2+1000\theta_4^2$$

<font color=00cccc size=4>只要让后两项接近于0，整个函数就几乎可以看作是一个二次函数。</font>  

后两项也被成为<font color=ff0099 size=4>惩罚项</font>。  

所以正则化的思路可以整理为：  
- 简化假设函数，使其大体上应该是一个**二次函数**。
  
例如：  
关于房价预测，有特征变量$x_1,x_2,\cdots,x_100$，那么也就相对应的有这么多参数$\theta_0,\theta_1,\cdots,\theta_100$。并且也无法知道应该剔除哪些特征变量，更不知道哪些是高次方项。我们能做的就是修改它的代价函数，这里是线性回归的代价函数，在其后方添加一个<font color=ff0099 size=4>正则化项</font>以缩小所有的参数值:  
<font color=00cccc size=4>

$$J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\quad+\lambda\sum\limits_{i=1}^m\theta_j^2$$

</font>

- 通常情况下，我们只对$\theta_1$开始的参数进行正则化，而不会对$\theta_0$进行，并且这对结果的影响通常可以忽略不计。
- 后边加的那一项就是<font size=4 color=ff0099>正则化项</font>，而$\lambda$被称为<font color=ff0099 size=4>正则化参数</font>。$\lambda$ 的作用就是控制在两个不同目标中的平衡关系——第一个目标是要使假设更好的拟合训练数据，第二个目标是尽量保持参数值较小,从而保持假设的形式相对简单，来避免过度拟合的问题。
- 如果$\lambda$数值过大，会导致拟合不足，甚至是无法拟合。
- 其实$\lambda$也可以像学习率一样由算法自动选择。
<br />
<br />
<br />
<br />

# 正则化线性回归(regularized Linear Regression)

对于线性回归的两种算法——梯度下降和正规方程，同样也适用于正则化线性回归。  

这是之前推导出的正则化线性回归：

<font color=ff6699 size=4>

$$
J(\theta)=\frac{1}{2m}\bigg [\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum\limits_{i=1}^m\theta_j^2\bigg ]$$
</font>  

目标是找到参数$\theta$能最小化代价函数。  

并且之前也使用没有正则化的梯度下降：  
$$Repeat\bigg\{\\
\theta_j:=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\\
(j=0,1,2,3,\cdots,n)\\
\bigg\}$$

因为在正则化的惩罚项中不包含$\theta_0$,所以要把它单独写出来，以方便正则化对剩下的$\theta_1,\theta_2,\cdots,\theta_n$起作用：

$$
Repeat\bigg\{\\
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\
\theta_j:=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\\
(j=1,2,3,\cdots,n)$$

下一步就是给表达式添加<font color=ff0099 size=4>正则化项</font>并得出最终的正则化梯度下降:

<font color=ff6699 size=4>

$$
Repeat\bigg\{\\
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\
\theta_j:=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\ \ +\frac{\lambda}{m}\theta_j\\
(j=1,2,3,\cdots,n)
$$

</font>

第二个表达式可以等价的写成如下形式：

<font color=ff6699 size=4>

$$\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$

</font>

- 通常来说$1-\alpha\frac{\lambda}{m}$是一个具体的正实数，并且$<1$。这样就使$\theta_j$变的小了一点。

<br />
<br />
<br />

# 正则化正规方程(Regularized Normal equation)

之前推导过的正规方程不带有正则化项：

$$
\theta=(X^TX)^{(-1)}X^Ty$$

添加正则化项之后的正规方程：

<font color=ff6699 size=4>

$$\theta=\bigg(X^TX+\lambda\left[
\begin{matrix}
0 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{matrix}
\right] \bigg)^{-1}X^Ty\\
矩阵为(n+1)\times(n+1)维。
$$

</font>

## 不可逆(Non-invertibility)

假设$m\leq n$，其中m是样本数量，n是特征变量的数量。那么$X^TX$结果将是不可逆的或奇异的(singluar)，或者说这个矩阵是退化的(degenerate)。这种情况下虽然用求伪逆可以获得一个解，但并不是一个可靠的结果。如果用求常规逆的方法就无法计算。  

### 正则化解决了这个问题。

<font color=00cccc size=4>加入正则化项的结果将是一个可逆的矩阵。</font>

<br />
<br />
<br />
<br />

# 正则化逻辑回归(regularized Logistic Regression)

之前已经推导过的逻辑回归代价函数：

$$
J(\theta)=-\bigg[\frac{1}{m}\sum\limits_{i=1}^m y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-(x^{(i)}))
\bigg]
$$

并且：
$$
h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_1^2+\theta_3x_1^2x_2+\theta_4x_1^2x_2^2+\theta_5x_1^2x_2^3+\cdots)
$$

可以通过在表达式的末尾加上正则化项来实现<font color=ff0099 size=4>正则化逻辑回归代价函数</font>：

<font color=00cccc size=4>

$$
J(\theta)=-\bigg[\frac{1}{m}\sum\limits_{i=1}^m y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-(x^{(i)}))
\bigg]\quad+\frac{\lambda}{2m}\sum\limits_{j=1}^n\theta_j^2
$$
</font>

因为正则化排除了$\theta_0$，所以运行范围是$\theta_1,\theta_2,\cdots,\theta_n$。

根据之前线性回归的正则化经验，可以得到如下正则化梯度下降：

<font color=00cccc size=4>

$$
Repeat\bigg\{\\
    \theta_0=\theta_0-\alpha\frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\
    \theta_j:=\theta_j-\alpha\bigg[\frac{1}{m}\sum\limits_{i=1}^m(h_\theta (x^{(i)})-y^{(i)})x_j^{(i)}\quad+\frac{\lambda}{m}\theta_j\bigg]\\
    (j=1,2,3,\cdots,n)\\
    \bigg\}
$$

</font>