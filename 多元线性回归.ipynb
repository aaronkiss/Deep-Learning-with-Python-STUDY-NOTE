{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exact-somewhere",
   "metadata": {},
   "source": [
    "# 多元线性回归(Multivariate Linear Regression)\n",
    "\n",
    "在之前的例子中只有一个变量$x$\n",
    "\n",
    "\n",
    "| Size | Price |\n",
    "| ---- | ----- |\n",
    "|   x  |   y   |\n",
    "|2104|460|\n",
    "|1416|232|\n",
    "|1534|315|\n",
    "|852|178|\n",
    "|...|...|\n",
    "\n",
    "$$h_\\theta=\\theta_0+\\theta_1x$$  \n",
    "  \n",
    "现在需要有更多变量来帮助预测房价：\n",
    "\n",
    "|Size|Number of Bedrooms|Number of Floors|Age of home|Price|\n",
    "|---|---|---|---|---|\n",
    "|$x_1$|$x_2$|$x_3$|$x_4$|$y$|\n",
    "|2104|5|1|45|460|\n",
    "|1416|3|2|40|232|\n",
    "|1534|3|2|30|315|\n",
    "|852|2|1|36|178|\n",
    "|...|...|...|...|...|\n",
    "\n",
    "- 我们用$n$来表示特征的数量，在这里$n=4$。\n",
    "- $x^{(i)}$表示输入训练数据里第$i$个样本的数据（包含全部4个变量）。\n",
    "- $x^{(i)}_j$表示输入训练数据里第$i$个样本的第$j$个特征量。  \n",
    "所以，$x^{(2)}=\\left[\\begin{matrix}1416\\\\3\\\\2\\\\40\\end{matrix}\\right]$，并且$x^{(2)}_3=2$。  \n",
    "\n",
    "对于假设函数，之前的一元函数探讨的是$h_\\theta(x)=\\theta_0+\\theta_1x$。  \n",
    "在多元函数中，假设函数就要改写为：\n",
    "$h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$   \n",
    "假设$x_0=1$，那么$x^{(i)}_0=1$。  \n",
    "\n",
    "所以此时样本的特征向量为$x=\\left[\\begin{matrix}x_0\\\\x_1\\\\x_2\\\\ \\vdots \\\\x_n\\end{matrix}\\right]\\in R^{n+1}$，并且$\\theta=\\left[\\begin{matrix}\\theta_0\\\\\\theta_1\\\\\\theta_2\\\\\\vdots\\\\\\theta_n\\end{matrix}\\right]\\in R^{n+1}$。  \n",
    "### 再整理一次思路：在只有一个变量的时候可以用一元一次函数来探讨，就是$y=ax+b$的样子。当有多个变量的时候就可以直接在一元一次函数的基础上累加$y=ax_1+ax_2+\\cdots+ax_n+b$的样子。如果每次a的值都不一样，那就可以写成$y=a_1x_1+a_2x_2+\\cdots+a_nx_n+b$。公式就是这么来的了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-plate",
   "metadata": {},
   "source": [
    "所以把$x_0=1$代入上边的假设函数，就得到了 \n",
    "<font color=6600ff size=4>$$h_\\theta(x)=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_nx_n=\\theta^Tx$$</font>，$\\theta^T$是$\\theta$的**转置矩阵**（此处参考*向量内积*），也就是把本来为**列向量**的$\\theta$转置成为**行向量**。  \n",
    "- 向量内积公式：$a\\times b=a_1b_1+a_2b_2+a_3b_3+\\cdots+a_nb_n$  \n",
    "\n",
    "把$\\theta^Tx$展开就是$$\\left[\\begin{matrix}\\theta_0 & \\theta_1 & \\theta_2 & \\cdots & \\theta_n\\end{matrix}\\right]\\times\\left[\\begin{matrix}x_0\\\\x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{matrix}\\right]$$\n",
    "。根据矩阵乘法可以发现，不转置$\\theta$也不行。  \n",
    "### 具有多个变量的线性回归被称为多元线性回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-internet",
   "metadata": {},
   "source": [
    "# 多个变量的梯度下降(Gradient Descent for Multiple Variables)\n",
    "\n",
    "按照前例，有一个假设函数：$h_\\theta(x)=\\theta^Tx=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_nx_n$，并且$x_0=1$。  \n",
    "参数为$\\theta$的$n+1$维向量$[\\theta_0 \\quad \\theta_1 \\quad \\theta_2 \\quad \\cdots \\quad \\theta_n]$。  \n",
    "代价函数为$J(\\theta_0,\\theta_1,\\cdots,\\theta_n)=\\frac{1}{2m}\\sum\\limits^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2$  \n",
    "梯度下降：Repeat$\\left\\{\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}  \n",
    "{\\partial\\theta_j}J(\\theta_0,\\cdots,\\theta_n)\\right\\}\\quad(同时更新每一个J在0,1,\\cdots,n范围内)$。  \n",
    "当$n=1$时，Repeat$\\bigg\\{\\\\\n",
    "\\theta_0:=\\theta_0-\\alpha\\underbrace{\\frac{1}{m}\\sum\\limits^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})}_{\\frac{\\partial}{\\partial\\theta_0}J(\\theta)}\\\\\n",
    "\\theta_1:=\\theta_1-\\alpha\\frac{1}{m}\\sum\\limits^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}\\\\\n",
    "(simultaneously\\ update\\ \\theta_0,\\theta_1)\\\\\n",
    "\\bigg\\}$  \n",
    "其中$\\frac{\\partial}{\\partial\\theta_0}$括起来的部分时代价函数里部分求导的结果，就是代价函数相对于$\\theta_0$的偏导数。  \n",
    "当$n\\geq1$时：  \n",
    "Repeat$\\bigg\\{\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum\\limits^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j\\quad(simultaneously\\ update\\ \\theta_j for j=0,\\cdots\\,n)\n",
    "\\bigg\\}$  \n",
    "因为$x_0=1$，所以实际上它们三个是一回事。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-article",
   "metadata": {},
   "source": [
    "## 梯度下降  特征缩放(Gradient Descent  - Feature Scaling)\n",
    "\n",
    "如果一个机器学习问题有多个特征，并且这些特征的不同取值都处于一个**近似范围**，那么梯度下降法就能更快的收敛。  \n",
    "\n",
    "Idea: Make sure features are on a similar scale.  \n",
    "E.g.$x_1=size(0-2000\\ feet^2)\\\\x_2=number\\ of\\ bedrooms(1-5)$  \n",
    "<font color=ff0066 size=3>这样的取值范围会严重拖慢梯度下降的运算速度，因此要对数据进行**特征缩放**。</font>  \n",
    "于是就有了：$$x_1=\\frac{size(feet^2)}{2000}\\\\x_2=\\frac{number\\ of\\ bedrooms}{5}$$  \n",
    "经过特征缩放后的取值会更快的进行梯度下降运算。  \n",
    "通常在特征缩放时会把取值限制在$-1\\leq x_i\\leq 1$的范围内。-1和+1并不是必须的，也可以是-2和+0.25。  \n",
    "但是-100和+100就太夸张了。同样的，-0.00001和+0.00002也太夸张。**夸张的特征范围是不能用的。**  \n",
    "## 均值归一化(Mean normalization)\n",
    "\n",
    "使用$x_i-\\mu_i$来代替$x_i$得到近似0的均值。  \n",
    "$x_0=1$的情况无效。  \n",
    "E.g. $$x_1=\\frac{size-1000}{2000}\\\\x_2=\\frac{\\#bedrooms-2}{5}$$  \n",
    "所以公式为：<font color=9900ff size=4>$$x_i=\\frac{x_i-\\mu_i}{S_i}$$</font>  \n",
    "\n",
    "其中$\\mu_i$是所有特征值的平均数，$S_i$是值的范围（max-min）或标准偏差。  \n",
    "- 除以范围和除以标准差会得到**不同的结果**。\n",
    "例如：如果$x_i$代表房子价格的范围为100-2000，并且平均值为1000，那么$x_i:=\\frac{price-1000}{1900}$，其中1900=2000-100，最大值减最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-worthy",
   "metadata": {},
   "source": [
    "## 学习率(Learning Rate)\n",
    "\n",
    "学习率是梯度下降算法中的**更新规则**。  \n",
    "<font color=9900ff>\n",
    "- 如果$\\alpha$学习率太小，则收敛缓慢。\n",
    "- 如果$\\alpha$学习率太大，则$J(\\theta)$的值在每一次迭代中可能不能下降；无法收敛。    \n",
    "</font>  \n",
    "<font color=ff6699 size=4>我的思考：如果在每一次迭代后都用本次迭代输出值与上一次输出值进行比较，如果本次的小于上次的，那么就保持学习率不变；如果相反，就自动调低学习率，直到恢复“本次小于上次”的关系为止。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-breeding",
   "metadata": {},
   "source": [
    "## 特征和多项式回归(Features and Polynomial Regression)\n",
    "\n",
    "假设房子价格预测有两个特征：临街宽度(frontage)、纵深度(depth)，于是假设函数就可以写成：  \n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1\\times\\underbrace{ frontage}_{x_1}+\\theta_2\\times\\underbrace{ depth}_{x_2}$$  \n",
    "\n",
    "不一定非要用两个特征，也可以创造一个新特征$Area=x_1\\times x_2$,于是假设函数就可以写成：\n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1\\times Area$$  \n",
    "<font color=9900ff size=4>如何取舍在于从哪个角度审视问题，没有一定之规。</font>  \n",
    "  \n",
    "**如果直线的假设函数不能很好的拟合数据，则可以不用直线函数。**  \n",
    "可以通过将假设函数设为二次、三次或平方根函数（或其他的任何函数）来更改假设函数的曲线和行为。  \n",
    "考虑到二次函数图像会在后半段“降下来”，因此不适合作预测（也可能是很少有人经历房价跳水的时代），而直线无法做到更高程度的拟合，因此可以使用**3次方函数**来拟合。  \n",
    "假设函数可以写为：\n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3$$  \n",
    "可以看成：$x_1=(size),\\ x_2=(size)^2,\\ x_3=(size)^3$。  \n",
    "<font color=9900ff size=4>在这种情况下，均值归一化就更加重要！！</font>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-dictionary",
   "metadata": {},
   "source": [
    "# 正规方程(Normal Equation)\n",
    "\n",
    "对于某些线性回归问题，它给出了一个更好的方法——求出参数**$\\theta$**的最优解。  \n",
    "之前使用**多次迭代**的算法来计算梯度下降，进而得到全局最小值。而正规方程使用**解析**解法可以直接**一次性**求得$\\theta$的最优解。  \n",
    "\n",
    "假设有$J(\\theta)=a\\theta^2+jb\\theta+c$，如果$\\theta$是一个标量（即实数）而不是一个n+1维的向量，那么可以用求导的方式获得使$J(\\theta)$获得最小值的$\\theta$值。  \n",
    "如果$\\theta$是一个n+1维的向量呢？  \n",
    "\n",
    "|Size|Number of Bedrooms|Number of Floors|Age of home|Price|\n",
    "|---|---|---|---|---|\n",
    "|$x_1$|$x_2$|$x_3$|$x_4$|$y$|\n",
    "|2104|5|1|45|460|\n",
    "|1416|3|2|40|232|\n",
    "|1534|3|2|30|315|\n",
    "|852|2|1|36|178|\n",
    "|...|...|...|...|...|  \n",
    "\n",
    "仍然是开头的多特征变量房价的例子，它有4个训练样本（m=4），并且这也是全部的训练数据。  \n",
    "那么可以在样本前加上额外的特征变量**$x_0$**，表格如下：  \n",
    "\n",
    "|    |Size|Number of Bedrooms|Number of Floors|Age of home|Price|\n",
    "|---|---|---|---|---|---|\n",
    "| ---: | ---: | ---: | ---: | ---: | ---: |\n",
    "|$x_0$|$x_1$|$x_2$|$x_3$|$x_4$|$y$|\n",
    "|1|2104|5|1|45|460|\n",
    "|1|1416|3|2|40|232|\n",
    "|1|1534|3|2|30|315|\n",
    "|1|852|2|1|36|178|\n",
    "  \n",
    "于是就得到一个矩阵$X=\\left[\\begin{matrix}1 & 2104 & 5 & 1 & 45\\\\1 & 1416 & 3 & 2 & 40\\\\1 & 1534 & 3 & 2 & 30\\\\1 & 852 & 2 & 1 & 36\\end{matrix}\\right]$，并且 $y=\\left[\\begin{matrix}460\\\\232\\\\315\\\\178\\end{matrix}\\right]$。  \n",
    "所以$X$就是一个$m\\times (n+1)$维的矩阵，在这里是$4\\times5$维的矩阵；而$y$就是一个$m$维的向量，在这里是$4$维向量。  \n",
    "因此就有公式：\n",
    "<font color=ff0099 size=5>$$\\theta=(X^TX)^{-1}X^Ty$$</font>\n",
    "  \n",
    "### 这样就能得到能使代价函数$J(\\theta)$最小化的$\\theta$值。\n",
    "  \n",
    "## 思路整理\n",
    "假设有$m$个训练样本（相当于矩阵的行数），有$n$个特征变量（相当于矩阵的列数），所以$x_{(i)}$看起来更像个向量：$x_{(i)}=\\left[\\begin{matrix}x_0^{(i)}\\\\x_1^{(i)}\\\\x_2^{(i)}\\\\\\vdots\\\\x_n^{(i)}\\end{matrix}\\right]\\in R^{n+1}$。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-round",
   "metadata": {},
   "source": [
    "那么就可以用这个向量的转置矩阵（就是把向量“横过来”）来表示矩阵$X$：  \n",
    "$$\n",
    "\\left[\\begin{matrix}(x^{(1)})^T\\\\(x^{(2)})^T\\\\(x^{(3)})^T\\\\\\vdots\\\\(x^{(m)})^T\\end{matrix}\\right]\n",
    "$$  \n",
    "\n",
    "例如：如果$x^{(i)}=\\left[\\begin{matrix}1\\\\x^{(i)}_1\\end{matrix}\\right]$，相当于所有训练样本的第一个特征变量，那么就可以把它写成：\n",
    "$X=\\left[\\begin{matrix}1 & x^{(1)}_1\\\\1 & x^{(2)}_1\\\\\\vdots & \\vdots\\\\1 & x^{(m)}_1\\end{matrix}\\right]\\quad y=\\left[\\begin{matrix}y^{(1)}\\\\y^{(2)}\\\\\\vdots\\\\y^{(m)}\\end{matrix}\\right]$。  \n",
    "\n",
    "<!--  此处隐藏，因为不确定推导是否正确。\n",
    "不考虑矩阵的第1列的话，$X^T\\times X$不就是相当于所有样本的第1个特征变量的平方和嘛。写出来大概是这样的：$\\sum\\limits_{i=1}^4 (x^{(i)}_1)^2$，那么$X^TX=\\left[\\begin{matrix}\\sum\\limits^4_{i=1}x_1^{(i)} & \\sum\\limits^4_{i=1}(x_1^{(i)})^2\\\\4 & \\sum\\limits^4_{i=1}x_1^{(i)}\\end{matrix}\\right]$  \n",
    "\n",
    "那么$(X^TX)^{-1}=\\frac{1}{(\\sum\\limits^4_{i=1}x_1^{(i)})^2-4\\sum\\limits^4_{i=1}x_1^{(i)}}\\times \\left[\\begin{matrix}\\sum\\limits^4_{i=1}x_1^{(i)} & -\\sum\\limits^4_{i=1}(x_1^{(i)})^2\\\\\n",
    "-4 & \\sum\\limits^4_{i=1}x_1^{(i)}\\end{matrix}\\right]$。  \n",
    "-->  \n",
    "代码：  \n",
    "```octave\n",
    "pinv(x'*x)*x'*y\n",
    "\n",
    "```  \n",
    "\n",
    "# 两种算法对比\n",
    "\n",
    "如果拥有$m$个训练样本，有$n$个特征变量：  \n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>学梯度下降</th>\n",
    "<th>正规方程</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody><tr>\n",
    "<td>需要选择学习率$\\alpha$</td>\n",
    "<td>不需要选择学习率$\\alpha$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>需要很多迭代</td>\n",
    "<td>不需要迭代</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$n$很大时也可以工作正常</td>\n",
    "<td>$n$很大时计算会很慢</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "<font color=9900ff size=3>什么是$n$很大？如果$n$达到了10,000就是很大，但也可以通过逆变换成为一个$10000\\times10000$的矩阵解决问题，但如果是$n=10^6$就必须考虑梯度下降了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-hotel",
   "metadata": {},
   "source": [
    "| |梯度下降法|正规方程法|\n",
    "|:---:|:---:|:---:|\n",
    "|学习速率$\\alpha$|需要设置|不需要|\n",
    "|计算次数|需要多次迭代|不需要迭代|\n",
    "|特征归一化|需要|不需要|\n",
    "|时间复杂度|O(kn2)|O(n3)，需要计算$X^TX$的逆|\n",
    "|特征数量|即使n很大也可工作|如果n很大，计算缓慢|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-compression",
   "metadata": {},
   "source": [
    "|Gradient Descent|Normal Equation|\n",
    "|:-:|:-:|\n",
    "|Need to choose alpha|NO need to choose alpha|\n",
    "|Needs many Iterations|NO need to Iterate|\n",
    "|Need Mean Normalization|NO need Mean Normalization|\n",
    "|$O(kn^2)$|$O(n^3)$, need to calculate invers of $X^TX$|\n",
    "|Works WELL when $n$ is large|SLOW if $n$ is very large|\n",
    "\n",
    "<font color=ff0099 size=5>当n超过10000时，可能是从正规方程转到梯度下降的好时机。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-denmark",
   "metadata": {},
   "source": [
    "# 正规方程不可逆\n",
    "\n",
    "在Octave中实现正规方程时，应该使用**pinv**函数而不是inv函数。即便$X^TX$是不可逆的，**pinv**都可以计算出$\\theta$值。  \n",
    "\n",
    "不可逆的常见原因：  \n",
    "- 冗余特征，其中两个特征密切相关，即他们**线性相关**。*解决方法：删除与另一个线性相关的特征。*\n",
    "- 特征太多，例如$m\\leq n$。*解决方法：删除某些功能或使用**正则化**来解决。*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-machine",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
