使用学习到的参数实现神经网络的**推理处理**，被称为神经网络的**前向传播**（forward propagation)。

### MNIST数据集
训练图像6万张，测试图像1万张，28x28像素灰度图像（1通道），像素取值0～255之间。

```python
import numpy as np
from mnist import load_mnist
from PIL import Image


def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()

(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
#load_mnist函数以“训练图像”、“训练标签”、“测试图像”、“测试标签”的形式返回读取的MNIST数据。

print(x_train.shape)    #返回（60000，784）
print(t_train.shape)    #返回（60000）
print(x_test.shape)     #返回（10000，784）
print(t_test.shape)     #返回（10000）
```

注释：
    `normalize=True`表示将输入图像**正规化**为0.0～1.0之间的值，若设置为`False`表示图像保持0～255之间的值。  
    `flatten=True`表示将输入的图像**展开**成一条**一维数组**，若设置为`False`表示图像保持1x28x28的三维数组。而设置为`True`则表示图像被展开成为784（*1x28x28=784*）个元素组成的一维数组。  
    `one_hot_label`表示将标签保存为one hot表示（*one-hot representation 一站式代表*）。  
    **one-hot** 表示仅正确解标签为**1**，其余皆为**0**的数组，例如[0,0,0,1,0,0,0,0]。
    
### 显示MNIST图像
图像的显示使用**PIL**（Python Image Library）模块。

```python
img = x_train[0]
label = t_train[0]
print(label)

print(img.shape)

img = img.reshape(28,28)    #把图像变回原来的样子
print(img.shape)

img_show(img)    #显示图像
```
#### 在这个例子里，`img.reshape`相当于上文中`flatten=True`的逆向操作。


### 下面要进行的是对数据的推理处理

整理思路：

输入层有784个神经元（1x28x28=784），输出层有10个神经元（分类问题，输出层神经元个数取决于要分类别的数量，这里是10个）。  
神经网络有2个隐藏层。第一个隐藏层有50个神经元，第二个隐藏层有100个神经元。

```python
import pickle

#定义sigmoid函数
def sigmoid(x):
    return 1.0 / (1 + np.exp(-x))
    
def get_data():
    (x_train,t_train),(x_test,t_test) = load_mnist(normalize=True,flatten=True,one_hot_label=False)
    return x_test, t_test

def init_network():
    with open("sample_weight.pkl",'rb') as f:
        network = pickle.load(f)
    
    return network
#init_network函数会读入保存在pickle文件sample_weight.pkl中学习到的权重参数。


def predict(network, x):
    W1,W2,W3 = network['W1'], network['W2'], network['W3']
    b1,b2,b3 = network['b1'], network['b2'], network['b3']
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    z3 = sigmoid(a3)
    y = softmax(a3)
    
    return y

def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y


```
#### Python有pickle功能，可以将程序运行中的对象保存为文件，方便程序的快速访问和运算。

```python

x, t = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p = np.argmax(y)    #获得概率最高的元素索引
    if p == t[i]:
        accuracy_cnt += 1
        
print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

```

#### 输出结果：Accuracy:0.9352

经过了反复报错和反复排查及补充函数定义后，终于运算出了结果。  
这个例子的盲点，我感觉是`sample_weighjt.pkl`文件是书里给的现成权重文件，并不是我通过代码**训练**出来的。  
仅仅是熟悉了一下神经网络的计算过程。

#### 整理思路：  
先获得MNIST数据集，生成网络。*（下锅前先要拿到土豆）*  
然后用`for`语句逐一取出保存在**x**中的图像数据，用`predict()`函数以**Numpy数组**的形式输出各个标签的对应**概率**。*（把土豆摆开来检查一下看看哪些能吃）*  
接着用`np.argmax（x）`函数取出这个概率列表里的最大值的索引作为预测结果。*（把能吃的土豆拿出来）*  
最后比较神经网络所预测的答案与正确答案的标签，将回答正确的概率作为**辨识精度**。*（把挑选的土豆再检查一次确认没有挑错）*  

### 关于批处理 batch  

代码：  
```python

#接上文代码

x, t = get_data()
network = init_network()

batch_size = 100 # 指定批数量，为新增代码
accuracy_cnt = 0

for i in range(0, len(x), batch_size):    #range(m, n, s)表示m到n-1之间以s为步数的数列
    x_batch = x[i:i+batch_size]        #表示取值为i到i+batch_n之间的数据
    y_batch = predict(network, x_batch)    #predict()函数以numpy数组的形式输出个标签对应的概率
    p = np.argmax(y_batch, axis=1)        #此处的axis=1的等号两边不能有空格！！    axis=1表示沿着第1维（横向）方向找最大值的元素索引。
    accuracy_cnt += np.sum(p == t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

```
