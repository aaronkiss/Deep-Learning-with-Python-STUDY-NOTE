## 线性代数
### 线性方程
**一次线性方程**————所有变量都是一次方。如： h<sub>v</sub>(x) = v<sub>0</sub> + v<sub>1</sub> · x     **单变量线性回归**  

#### 代价函数(Cost Function)
————**平方误差函数(Squared Error Function)**，是解决回归问题最常用的手段。  
### *代价函数(Cost Function)就是损失函数(Loss Function)的函数！！！*  
>假设有训练样本(x, y)，模型为h，参数为θ。  
>
>h(θ) = θ<sup>T</sup> · x  
>
>（θT表示θ的转置），总体来讲，任何能够衡量模型预测出来的值h(θ)与真实值y之间的差异的函数都可以叫做代价函数C(θ)，如果有多个样本，则可以将所有代价函数的取值求均值，记做J(θ)。
>损失函数（Loss Function ）是定义在**单个样本**上的，算的是一个样本的误差，而代价函数（Cost Function ）是定义在**整个训练集**上的，是所有样本误差的平均，也就是损失函数的平均，这给我们评价学习效果提供了一个标准。代价函数越小说明模型和参数越符合训练样本(x, y)。
>对于不同的算法来说，适合他们的代价函数也不同。下面是一些常用的代价函数。
>
>[原文链接](https://blog.csdn.net/haha0825/article/details/102491643)
  
  
*由于此处无法显示公式，所以后续内容已经转为在Jupyternotebook上编辑*
